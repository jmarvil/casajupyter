{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee47f12-03c2-49e3-ac1b-c8604ecb97e9",
   "metadata": {},
   "source": [
    "# Tclean mpi examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0541ad3d-4c0f-4136-9c7a-cb00700a50f9",
   "metadata": {},
   "source": [
    "This  notebook contains two examples of running CASA's tclean task in parallel:\n",
    "* [Example 1](#example_1) - notebook server and MPI processes on the same single node\n",
    "* [Example 2](#example_2) - notebook server and ip controller on one node, MPI processes on one or more other nodes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc16c8b9-2b5c-4198-929d-67ca546f10b2",
   "metadata": {},
   "source": [
    "## Initial setup\n",
    "Here are the steps I followed to setup and start this notebook server:\n",
    "\n",
    "1\\. begin with a monolithic CASA distribution in the current directory, e.g. casa-6.4.3-27\n",
    "\n",
    "2\\. create a virtual environment based on CASA's distributed version of Python\n",
    "\n",
    "`casa-6.4.3-27/bin/python3 -m venv --system-site-packages venv`\n",
    "\n",
    "3\\. start the virtual environment \n",
    "\n",
    "`source venv/bin/activate`\n",
    "\n",
    "4\\. install additional packages\n",
    "\n",
    "`pip install jupyterlab ipyparallel`\n",
    "\n",
    "5\\. start jupyter lab\n",
    "\n",
    "`PATH=casa-6.4.3-27/lib/mpi/bin:$PATH JUPYTER_CONFIG_DIR=\".\" IPYTHONDIR=\".\" jupyter lab --no-browser --ip 0.0.0.0`\n",
    "\n",
    "Some notes:  The --system-site-packages flag allows the virtual environment to link to packages installed in the parent environment, i.e. those included in the CASA distribution. Modifying the PATH allows mpiexec, etc. from the CASA distribution to be found. Modifying the IPython and Jupyter config directory overrides any user-specific settings in $HOME (if any). Starting Jupyter with --ip 0.0.0.0 allows connections from other IP addresses.\n"
   ]
  },
  {
   "source": [
    "### Alternate setup\n",
    "\n",
    "The following procedure is another way to combine CASA with Jupyter and is more consistent with the documentation for using modular CASA. Both tclean examples will also work with this method without any code changes. There are a couple caveats with this method: you may need to install the casadata package or create a config.py with paths to the data folders, and you will need to install or locate openmpi (I am unsure about other flavors like mpich). At NRAO the version that worked best for me was in /opt/casa/03/bin.  Avoid versions in /usr/lib64 as they may be inconsent across RHEL7 machines.   \n",
    "\n",
    "1\\. create a virtual environment based on python 3.6 or 3.8. At NRAO these are in /opt/local/bin.\n",
    "\n",
    "`python3 -m venv venv`\n",
    "\n",
    "2\\. activate the environment\n",
    "\n",
    "`source venv/bin/activate`\n",
    "\n",
    "3\\. upgrade the version of pip \n",
    "\n",
    "`pip install --upgrade pip`\n",
    "\n",
    "4\\. install additional packages, providing the path to mpicc [as discussed here](https://open-bitbucket.nrao.edu/projects/CASA/repos/casampi/browse/README.md) \n",
    "\n",
    "`MPICC=/opt/casa/03/bin/mpicc pip install casatasks casampi jupyterlab ipyparallel`\n",
    "\n",
    "5\\. start jupyter lab\n",
    "\n",
    "`PATH=/opt/casa/03/bin:$PATH JUPYTER_CONFIG_DIR=\".\" IPYTHONDIR=\".\" jupyter lab --no-browser --ip 0.0.0.0`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "5817f75a-c6d3-4e2c-9409-38f0e4d422bd",
   "metadata": {},
   "source": [
    "## <a name=\"example_1\"></a>Example 1\n",
    "This is a simple example that starts 4 MPI processes on the local machine.  All processes need to then import start_mpi. The rank 0 process becomes the 'command client' and returns so that it can receive new commands while the other processes go into a serve() loop. This would typically happen automatically if the MPI processes imported either casatasks or casashell, but here we only import casatasks on the notebook kernel so we need to run this manually on the other processes. \n",
    "\n",
    "Then we issue the tclean command to the rank 0 process and it distributes work to the servers running the loop. Because of the 'with' statement, the controller and engines are automatically stopped when tclean is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6abf888-cf22-446d-883e-0fc769abad6d",
   "metadata": {},
   "outputs": [
        {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 4 engines with <class 'ipyparallel.cluster.launcher.MPIEngineSetLauncher'>\n",
      "100%|███████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.48s/engine]\n",
      "Stopping engine(s): 1646610038\n",
      "engine set stopped 1646610038: {'exit_code': 1, 'pid': 30849, 'identifier': 'ipengine-1646610037-togx-1646610038-30777'}\n",
      "Stopping controller\n",
      "Controller stopped: {'exit_code': 0, 'pid': 30814, 'identifier': 'ipcontroller-1646610037-togx-30777'}\n"
     ]
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "from casatasks import tclean\n",
    "\n",
    "with ipp.Cluster(engines=\"mpi\", n=4) as rc:\n",
    "    rc[:].execute('import casampi.private.start_mpi')\n",
    "    rc[0].apply_sync(tclean, vis='0420+417.ms', imagename='mpi_tclean_example1', niter=100, cell='1arcsec', parallel=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3006c8e-e450-46b7-b852-0de56d193a5a",
   "metadata": {},
   "source": [
    "## <a name=\"example_2\"></a>Example 2\n",
    "This example goes through the individual steps to start the controller and sync up the processes. This provides greater flexibility and would be better suited for an environment where a user has multiple clusters or profiles, and/or where a user wants to start MPI processes on other nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f5aeb-3850-468b-92c7-5448db8e6ffa",
   "metadata": {},
   "source": [
    "#### 2.1 define the cluster and start the controller\n",
    "\n",
    "Note: controller_ip=\"*\" tells the controller to accept connections from processes on other ip addresses.  Also, profile and cluster_id can be arbitrary names used to distinguish this notebook's resources from other clusters the user might be running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "524721aa-830c-4fda-8e87-43c28d2198c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Cluster(cluster_id='1234', profile='default', controller=<running>)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "\n",
    "cluster = ipp.Cluster(engines=\"mpi\", controller_ip=\"*\", profile='default', cluster_id='1234')\n",
    "await cluster.start_controller()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3f30f-05a3-4941-9078-c9963b97cf39",
   "metadata": {},
   "source": [
    "#### 2.2 start and connect the engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0776435-a929-429b-a04e-9e763c0460f6",
   "metadata": {},
   "source": [
    "To demonstrate multi-node MPI I created a hostfile listing each node and the number of MPI processes per node that I wanted to start.  At NRAO this required that I have an interactive reservation on each listed node and that I setup ssh keys.  I would like to get this working through torque or slurm in a future example. \n",
    "Note: The absolute path to mpiexec seems to be required below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ada2ed-6df2-42e9-a827-09ef55ec58d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.20s/engine]\n"
     ]
    }
   ],
   "source": [
    "with open('hosts.txt','w') as out1:\n",
    "    out1.writelines([\n",
    "        'nmpost035.aoc.nrao.edu slots=2 \\n',\n",
    "        'nmpost036.aoc.nrao.edu slots=3 \\n'\n",
    "    ])\n",
    "    \n",
    "from subprocess import Popen, PIPE    \n",
    "p = Popen(\"<full path to>mpiexec -x PATH -x IPYTHONDIR --hostfile hosts.txt \\\n",
    "          ipengine --file=profile_default/security/ipcontroller-1234-engine.json\".split(), stdout=PIPE, stderr=PIPE)\n",
    "\n",
    "rc =  await cluster.connect_client()\n",
    "rc.wait_for_engines(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a1716-c2a6-41c5-ba8b-c72a872adcfc",
   "metadata": {},
   "source": [
    "#### 2.3 map engine ID to MPI rank\n",
    "In Example 1 the engine ID and MPI rank always seemed to be equal so this step was not required, but here the engines can connect to the controller in an arbitrary order.  This code will fetch the MPI rank of each engine and use that to determine which engine has MPI rank 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7ab0734-6a2e-40da-8d5c-02aaf83a8025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine ID 1 has MPI rank 0\n"
     ]
    }
   ],
   "source": [
    "def get_mpi_rank():\n",
    "    from mpi4py import MPI\n",
    "    return MPI.COMM_WORLD.Get_rank()\n",
    "    \n",
    "rank = rc[:].apply_async(get_mpi_rank).get()\n",
    "rank0 = rank.index(0)\n",
    "print(f\"engine ID {rank0} has MPI rank 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4043d810-bfa9-4825-898f-a0f83d2c9cfa",
   "metadata": {},
   "source": [
    "#### 2.4 start mpicasa\n",
    "Import casampi on all the processes as in Example 1. The rank 0 process will become the CASA MPI command client, and the other processes will become CASA server processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bbd9d6b-b594-4ecd-aed7-585e29ffc0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult(execute): pending>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc[:].execute('import casampi.private.start_mpi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6134f-f4dd-45d1-a5f0-487fdac685c0",
   "metadata": {},
   "source": [
    "#### 2.5 run tclean\n",
    "Submit the tclean call to the rank 0 process and wait for it to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1045d93-6278-4768-b2c3-0f808e4ba6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from casatasks import tclean\n",
    "\n",
    "rc[rank0].apply_sync(tclean, vis='0420+417.ms', imagename='mpi_tclean_example2', niter=100, cell='1arcsec', parallel=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf7b6d-dccc-406f-9c23-7a0ca16e6aa7",
   "metadata": {},
   "source": [
    "#### 2.6 stop the engines and the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac5c6b7-f20b-4b3f-a494-77e03df9a0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping controller\n",
      "Controller stopped: {'exit_code': 0, 'pid': 31265, 'identifier': 'ipcontroller-1234-30777'}\n"
     ]
    }
   ],
   "source": [
    "p.terminate()\n",
    "await cluster.stop_cluster()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
